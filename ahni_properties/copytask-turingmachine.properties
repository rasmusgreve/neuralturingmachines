random.seed=1234567
#random.seed.simulator.fallback=1234567
run.name=tmaze-turingmachine-double
run.reset=true

# If set to "true" then substitutions present in property values will be enabled. Substitutions have the format $([key]), where [key] is the key of another property.
substitution.enable=true

#####################
# General stuff by us
#####################
simulate.generations.identical = -1

#####################
# Simulator and TMaze
#####################
simulator.class = dk.itu.ejuuragr.domain.CopyTask
simulator.copytask.length.max = 10
# "fixed" or "random" (fixed is bad)
simulator.copytask.length.rule = random
simulator.copytask.element.size = 4
# "strict-close", "emilarity", "closest-binary", "complete-binary" or "partial-score"
simulator.fitness.function = strict-close

################
# Turing Machine
################
tm.class = dk.itu.ejuuragr.turing.MinimalTuringMachine
tm.enabled = true
tm.n = 100
tm.m = 2
tm.shift.length = 3
# "multiple" or "single" (single is bad)
tm.shift.mode = multiple
tm.heads.readwrite = 1
tm.heads.read = 1
tm.heads.write = 1
tm.sharpening.factor = 1
controller.class = dk.itu.ejuuragr.turing.TuringController
controller.iterations = 1




###########
# evolution
###########	
num.runs=10
num.generations=500
popul.size=500

performance.target=1.0
performance.target.type=higher
# If greater than 1 then use an average of the best performance over this many generations.
performance.target.average=5

#true means mutation probabilities are applied to all possible places a mutation could occur
#false means probabilities apply to individual as a whole; only one topological mutation can occur per individual
#note that this applies only to topological mutations, not weight mutations
topology.mutation.classic=true

# Mutation rate for original NEAT add neuron topological mutation where a neuron replaces an existing connection. 
add.neuron.mutation.rate=0.1

add.connection.mutation.rate=0.5
#[0.01, 0.3]
remove.connection.mutation.rate=0.1
#only remove weights with magnitude smaller than this
remove.connection.max.weight=1

#should be 1.0
prune.mutation.rate=1.0

#[0.1, 0.8]. 0.5, 0.6
weight.mutation.rate=0.3
#[1.0, 2.0] dependent on weight.max/min?
weight.mutation.std.dev=3
# The amount to perturb weights by when generating the initial population. Default is weight.mutation.std.dev
#weight.mutation.std.dev.initial=0.5

#percent of individuals used as parents
survival.rate=0.3
#proportion of sexual (crossover) versus asexual reproduction
crossover.proportion=0.5

#[1, 5]
selector.elitism.min.specie.size=5
#percent of individuals from each species copied to next generation unchanged
selector.elitism.proportion=0.1
#min number to select from a species (if it has size >=  selector.elitism.min.specie.size)
selector.elitism.min.to.select=1
selector.roulette=false
selector.max.stagnant.generations=99999
selector.speciated.fitness=true


############
# speciation
############
#species distance factors
#c1, excess genes factor [1.0, 2.0]
chrom.compat.excess.coeff=2.0
#c2, disjoint genes factor [1.0, 2.0]
chrom.compat.disjoint.coeff=2.0
#c3, Weight difference factor [0.2, 3.0]
chrom.compat.common.coeff=1.0

#compatability threshold [0.1, 4.0], relative to c#
speciation.threshold=1.9
speciation.target=8


##################
# fitness function
##################
fitness_function.class=dk.itu.ejuuragr.fitness.HyperTMazeEvaluator
#max threads to use for fitness evaluation (including transcription of genotype/cppn to phenotype/substrate)
#if value is <= 0 then the detected number of processor cores will be used
fitness.max_threads=0
#if scale.factor > 1 then the substrate height, width and connection.range (if supported)
#will be multiplied by scale.factor every time scale.fitness is reached, at 
#most scale.times times.
fitness.hyperneat.scale.factor=0
fitness.hyperneat.scale.times=0
fitness.hyperneat.scale.performance=0.95
fitness.hyperneat.scale.recordintermediateperformance=true

################
# CPPN/AnjiNet #
################
#input and output size determined by hyperneat settings
#stimulus.size=7
#response.size=1
initial.topology.activation=random
initial.topology.fully.connected=true
initial.topology.num.hidden.neurons=0
initial.topology.activation.input=linear
initial.topology.activation.output=sigmoid
initial.topology.activation.random.allowed=sigmoid, gaussian, sine, absolute, linear, clamped-linear, sign
recurrent=disallowed
recurrent.cycles=1
#[1, 500]
weight.max=10
#weight.min=-3
bias.via.input=false

#############
# HyperNEAT #
#############

ann.transcriber.class=com.ojcoleman.ahni.transcriber.HyperNEATTranscriberBain
ann.transcriber.bain.executionmode=SEQ

ann.transcriber.neuron.model=com.ojcoleman.bain.neuron.rate.SigmoidNeuronCollection
ann.transcriber.synapse.model=com.ojcoleman.bain.synapse.rate.FixedSynapseCollection

#ann.transcriber.class=com.ojcoleman.ahni.transcriber.HyperNEATTranscriberGridNet
#ann.hyperneat.activation.function=sigmoid

ann.hyperneat.feedforward=true
#ann.hyperneat.cyclesperstep=4  not required for feed forward
ann.hyperneat.enablebias=true
ann.hyperneat.includedelta=true
ann.hyperneat.includeangle=false
ann.hyperneat.useinputlayerencoding=false

#ann.hyperneat.connection.expression.threshold=0.2
#ann.hyperneat.connection.range=2
ann.transcriber.connection.weight.min=-3
ann.transcriber.connection.weight.max=3

# If more than 0 you need three -1 values in height and width
ann.topology.num.hidden.neurons=0
ann.hyperneat.height=-1,-1
ann.hyperneat.width=-1,-1

#Should it be possible to make connections between input->output not going through the hidden layer (if present)
ann.transcriber.bain.substrate.connect.all=false

ann.hyperneat.range.x=-1,1
ann.hyperneat.range.y=-1,1
ann.hyperneat.range.z=-1,1


#############
# persistence
#############
persistence.class=com.anji_ahni.persistence.FilePersistence
persistence.base.dir=./db
persist.enable=false
persist.all=false
persist.champions=false
persist.last=false
persist.load.genotype=false
id.file=./db/id.xml
neat.id.file=./db/neatid.xml

##############
# presentation
##############
presentation.generate=false
presentation.dir=./nevt

#########
# logging
#########
output.dir=./ahniDB/$(run.name)
# How often to produce a line in the log containing a brief summary of the current progress.
log.pergenerations=1
# Whether to log the champ to a text file and/or image. N < 0 indicates no logging, N=0 indicates 
# only at the end of evolution, N > 0 indicates every N generations and after evolution has finished.
log.champ.tostring=100
log.champ.toimage=100


# FileAppenders with the name RunLog receive special treatment: for each run the output will be directed to a file 
# with the name specified by log4j.appender.RunLog.File in the directory [output.dir]/[run number]/
#log4j.rootLogger=INFO, C, RunLog
log4j.rootLogger=INFO, C, RunLog
log4j.appender.C=org.apache.log4j.ConsoleAppender
log4j.appender.RunLog=org.apache.log4j.FileAppender
log4j.appender.RunLog.File=log.txt
log4j.appender.C.layout=org.apache.log4j.PatternLayout
log4j.appender.RunLog.layout=org.apache.log4j.PatternLayout
log4j.appender.C.layout.ConversionPattern=%-5p %m%x%n
log4j.appender.RunLog.layout.ConversionPattern=%-5p %m%x%n



#######################################
# parameter tuning via ParameterTuner #
#######################################

parametertuner.numruns=40
parametertuner.numgens=50
parametertuner.solvedperformance=1.0
#parametertuner.htcondor=\
#  jar_files = ../../../lib/aparapi.jar ../../../lib/bain.jar ../../../lib/commons-lang3-3.1.jar ../../../lib/commons-math3-3.1.1.jar ../../../lib/jakarta-regexp-1.3.jar ../../../lib/jcommander.jar ../../../lib/log4j.jar ../../../lib/wildcard-1.03.jar
#  Rank                  = kflops \n \
#  +RequiresWholeMachine = True \n \
#  notification = Never

parametertuner.tune.0.prop=add.neuron.mutation.rate
parametertuner.tune.0.type=float
parametertuner.tune.0.adjust.type=delta
parametertuner.tune.0.adjust.amount=0.1
parametertuner.tune.0.initial=0.1
parametertuner.tune.0.max=1

parametertuner.tune.1.prop=add.connection.mutation.rate
parametertuner.tune.1.type=float
parametertuner.tune.1.adjust.type=delta
parametertuner.tune.1.adjust.amount=0.1
parametertuner.tune.1.initial=0.5
parametertuner.tune.1.max=1

parametertuner.tune.2.prop=weight.mutation.rate
parametertuner.tune.2.type=float
parametertuner.tune.2.adjust.type=delta
parametertuner.tune.2.adjust.amount=0.1
parametertuner.tune.2.initial=0.5

parametertuner.tune.5.prop=remove.connection.mutation.rate
parametertuner.tune.5.type=float
parametertuner.tune.5.adjust.type=delta
parametertuner.tune.5.adjust.amount=0.1
parametertuner.tune.5.initial=0.2
parametertuner.tune.5.max=1

parametertuner.tune.6.prop=initial.topology.fully.connected
parametertuner.tune.6.type=boolean
parametertuner.tune.6.initial=false

parametertuner.tune.7.prop=initial.topology.num.hidden.neurons
parametertuner.tune.7.type=integer
parametertuner.tune.7.adjust.type=delta
parametertuner.tune.7.adjust.amount=1
parametertuner.tune.7.initial=1
parametertuner.tune.7.max=4
