# This file contains general HyperNEAT settings, configured to use the plastic synapse model described by 
# A. Soltoggio, J. Bullinaria, C. Mattiussi, P. Durr, D. Floreano (2008) Evolutionary Advantages of 
# Neuromodulated Plasticity in Dynamic, Reward-based Scenarios.

ann.transcriber.class=com.ojcoleman.ahni.transcriber.HyperNEATTranscriberBain
#ann.transcriber.class=com.ojcoleman.ahni.transcriber.ESHyperNEATTranscriberBain
#ann.transcriber.bain.maxrecurrentcyclesearchlength=20
ann.transcriber.bain.executionmode=SEQ

ann.transcriber.neuron.model=com.ojcoleman.bain.neuron.rate.SoltoggioModulatoryNeuronCollection
ann.transcriber.synapse.model=com.ojcoleman.bain.synapse.rate.SoltoggioModulatorySynapseCollection

# Specifies that instead of parameter values being defined individually for each
# neuron, there will be a set of parameter value collections which can be thought of as defining a class of
# neurons as determined by the specific parameter values (which will determine the behaviour of that class of
# neuron). Each neuron then references one of these collections/classes and has its parameters set
# accordingly. If this is set to 0 or not specified then each synapse should be assigned parameters values directly
# (e.g. via an output for each parameter from a CPPN in a HyperNEAT encoding scheme), if this is set to a value
# greater than 0 then it specifies the number of classes.
#ann.transcriber.neuron.model.params.classes=4

ann.transcriber.neuron.model.types=modulatory,0,1

# Create CPPN outputs that set the parameters for each synapse.
ann.transcriber.synapse.model.params=n,a,b,c

# See ann.transcriber.neuron.model.params.classes
#ann.transcriber.synapse.model.params.classes=1

# Min and max values for model parameters.
ann.transcriber.synapse.model.params.min=-25,-1,-1,-1
ann.transcriber.synapse.model.params.max=25,1,1,1
ann.transcriber.synapse.model.params.expression.threshold=0.2,0.1,0.1,0.1
#ann.transcriber.synapse.model.params.expression.threshold=0.5,0.1,0.1,0.1
# This parameter in the synapse model will be set to 0 if the connection should not be expressed. This is typically applied to a "learning rate" parameter.
ann.transcriber.synapse.model.plasticitydisableparam=n

ann.hyperneat.feedforward=false
#For networks with recurrent connections, the number of activation cycles to perform each time the substrate network is presented with new input and queried for its output.
#For this experiment the network can decide when it's ready by setting the last output to a value greater than 0.5. 
ann.hyperneat.cyclesperstep=1
ann.hyperneat.enablebias=true
ann.hyperneat.includedelta=true
ann.hyperneat.includeangle=false
ann.hyperneat.useinputlayerencoding=true

ann.hyperneat.connection.expression.threshold=0.0
ann.hyperneat.leo=true
ann.hyperneat.leo.threshold=0.1
ann.hyperneat.leo.threshold.factordistance=false
ann.hyperneat.leo.threshold.directionalfactor=6,2,1
#ann.hyperneat.neo=true
#ann.hyperneat.neo.threshold=0

#ann.transcriber.connection.weight.min=-2
ann.transcriber.connection.weight.max=4

ann.hyperneat.range.x=-1,1
ann.hyperneat.range.y=-1,1
ann.hyperneat.range.z=-1,1


# ES-HypernNEAT params (only if ann.transcriber.class=com.ojcoleman.ahni.transcriber.ESHyperNEATTranscriberBain)
ann.eshyperneat.iterations=1
ann.eshyperneat.depth.initial=1
ann.eshyperneat.depth.max=2
ann.eshyperneat.division.threshold=0.5
ann.eshyperneat.variance.threshold=0.03
ann.eshyperneat.band.threshold=0.3
